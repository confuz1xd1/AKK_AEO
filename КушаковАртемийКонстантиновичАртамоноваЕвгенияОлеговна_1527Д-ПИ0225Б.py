# –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤
# –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Google Colab | TensorFlow 2.x | –†—É—Å—Å–∫–∏–µ –æ—Ç–∑—ã–≤—ã —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
# –¢–µ–º–∞: "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –±–∞–∑–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ TensorFlow Python
# –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —ç—Å—Å–µ –∏ –æ—Ç–∑—ã–≤–æ–≤ –æ –ª–µ–∫—Ü–∏—è—Ö"

# ===============================================
# 1. –£–°–¢–ê–ù–û–í–ö–ê –ò –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
# ===============================================
!pip install tensorflow transformers datasets scikit-learn matplotlib seaborn plotly kaleido

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
import re
from collections import Counter
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

print(f"TensorFlow –≤–µ—Ä—Å–∏—è: {tf.__version__}")
print(f"GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {tf.config.list_physical_devices('GPU')}")

# ===============================================
# 2. –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –†–£–°–°–ö–û–Ø–ó–´–ß–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê
# ===============================================
# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç RuSentiment —Å HuggingFace (—Ä—É—Å—Å–∫–∏–µ –æ—Ç–∑—ã–≤—ã –∏ —Ç–µ–∫—Å—Ç—ã)
dataset = load_dataset("k1tub/sentiment_dataset", split='train')

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ pandas –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
df = pd.DataFrame(dataset)
print("–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞:", df.shape)
print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
print(df['label'].value_counts())

# –ú–∞–ø–ø–∏–Ω–≥ –ª–µ–π–±–ª–æ–≤: 0=negative, 1=neutral, 2=positive
label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}
df['label_name'] = df['label'].map(label_map)

# –ë–µ—Ä–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ Colab (10k –ø—Ä–∏–º–µ—Ä–æ–≤)
df_sample = df.sample(n=10000, random_state=42).reset_index(drop=True)

# ===============================================
# 3. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–û–í
# ===============================================
def preprocess_text(text):
    """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    if pd.isna(text):
        return ""
    text = str(text).lower()
    # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –±–∞–∑–æ–≤—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    text = re.sub(r'[^–∞-—è—ëa-z0-9\s.,!?()-]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
df_sample['clean_text'] = df_sample['text'].apply(preprocess_text)

# –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
df_sample = df_sample[df_sample['clean_text'].str.len() > 10]
print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df_sample)} –ø—Ä–∏–º–µ—Ä–æ–≤")

# ===============================================
# 4. –°–û–ó–î–ê–ù–ò–ï –°–õ–û–í–ê–†–Ø –ò –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø
# ===============================================
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–ª–æ–≤–∞—Ä—è (–ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
VOCAB_SIZE = 10000
MAX_LEN = 100
EMBEDDING_DIM = 128

# –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å
all_words = []
for text in df_sample['clean_text']:
    all_words.extend(text.split())

word_counts = Counter(all_words)
most_common = word_counts.most_common(VOCAB_SIZE - 2)  # -2 –¥–ª—è padding –∏ unk
vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}
vocab['<PAD>'] = 0
vocab['<UNK>'] = 1

print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(vocab)} —Å–ª–æ–≤")

def text_to_sequence(text, vocab, max_len):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤"""
    words = text.split()
    seq = [vocab.get(word, vocab['<UNK>']) for word in words]
    # –û–±—Ä–µ–∑–∞–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ max_len
    if len(seq) >= max_len:
        seq = seq[:max_len]
    else:
        seq.extend([vocab['<PAD>']] * (max_len - len(seq)))
    return seq

# –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
X = np.array([text_to_sequence(text, vocab, MAX_LEN) for text in df_sample['clean_text']])
y = tf.keras.utils.to_categorical(df_sample['label'], num_classes=3)

# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=df_sample['label']  # –∑–¥–µ—Å—å –≤—Å—ë –æ–∫
)

# –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ –∂–µ –º–µ—Ç–∫–∏ df_sample, –∞ –Ω–µ df_temp
labels_temp = np.argmax(y_temp, axis=1)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size=0.25,          # 0.25 –æ—Ç 0.8 = 0.2, –∏—Ç–æ–≥–æ 60/20/20
    random_state=42,
    stratify=labels_temp     # –≤–º–µ—Å—Ç–æ df_temp['label']
)

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")


# ===============================================
# 5. –ü–û–°–¢–†–û–ï–ù–ò–ï –ü–û–õ–ù–û–°–í–Ø–ó–ù–û–ô –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò
# ===============================================
# –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
def create_model(vocab_size, embedding_dim, max_len, num_classes,
                hidden_units=[256, 128, 64], dropout_rate=0.3,
                embedding_trainable=True, l2_reg=1e-4):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –ù–° —Å –ø–æ–ª–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º
    """
    model = keras.Sequential([
        # –°–ª–æ–π –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è —Å–ª–æ–≤
        layers.Embedding(vocab_size, embedding_dim,
                        input_length=max_len,
                        trainable=embedding_trainable,
                        name='embedding'),

        # Global Average Pooling –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        layers.GlobalAveragePooling1D(name='global_avg_pool'),

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        layers.Dense(hidden_units[0], activation='relu',
                    kernel_regularizer=keras.regularizers.l2(l2_reg),
                    name='dense_1'),
        layers.BatchNormalization(name='bn_1'),
        layers.Dropout(dropout_rate, name='dropout_1'),

        layers.Dense(hidden_units[1], activation='relu',
                    kernel_regularizer=keras.regularizers.l2(l2_reg),
                    name='dense_2'),
        layers.BatchNormalization(name='bn_2'),
        layers.Dropout(dropout_rate, name='dropout_2'),

        layers.Dense(hidden_units[2], activation='relu',
                    kernel_regularizer=keras.regularizers.l2(l2_reg),
                    name='dense_3'),
        layers.Dropout(dropout_rate/2, name='dropout_3'),

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.Dense(num_classes, activation='softmax', name='output')
    ])

    return model

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
model = create_model(
    vocab_size=VOCAB_SIZE,
    embedding_dim=EMBEDDING_DIM,
    max_len=MAX_LEN,
    num_classes=3,
    hidden_units=[256, 128, 64],
    dropout_rate=0.3,
    embedding_trainable=True,
    l2_reg=1e-4
)

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º AdamW –∏ focal loss
model.compile(
    optimizer=keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# ===============================================
# 6. CALLBACKS –î–õ–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –û–ë–£–ß–ï–ù–ò–Ø
# ===============================================
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=5,
        restore_best_weights=True,
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-7,
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        'best_sentiment_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
]

# ===============================================
# 7. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# ===============================================
print("\nüöÄ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò...")
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=128,
    callbacks=callbacks,
    verbose=1
)

# ===============================================
# 8. –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò
# ===============================================
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nüéØ –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.4f}")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred_proba = model.predict(X_test)
y_pred = np.argmax(y_pred_proba, axis=1)
y_true = np.argmax(y_test, axis=1)

# –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
print("\nüìä –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–û–ù–ù–´–ô –û–¢–ß–ï–¢:")
print(classification_report(y_true, y_pred,
                          target_names=['negative', 'neutral', 'positive']))

# ===============================================
# ===============================================
# 9. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–ì–õ–ê–î–ö–ò–ï –ì–†–ê–§–ò–ö–ò)
# ===============================================

train_acc  = np.array(history.history['accuracy'])
val_acc    = np.array(history.history['val_accuracy'])
train_loss = np.array(history.history['loss'])
val_loss   = np.array(history.history['val_loss'])

# —Ä–µ–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö —ç–ø–æ—Ö
n_epochs = len(train_acc)
epochs_range = np.arange(1, n_epochs + 1)

# --- —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏–º —Å—Ä–µ–¥–Ω–∏–º ---
def smooth_curve(x, window=3):
    if window <= 1:
        return x
    x_pad = np.pad(x, (window-1, 0), mode='edge')
    kernel = np.ones(window) / window
    return np.convolve(x_pad, kernel, mode='valid')

smooth_train_acc  = smooth_curve(train_acc,  window=3)
smooth_val_acc    = smooth_curve(val_acc,    window=3)
smooth_train_loss = smooth_curve(train_loss, window=3)
smooth_val_loss   = smooth_curve(val_loss,   window=3)

fig, axes = plt.subplots(1, 2, figsize=(14, 4))

# --------- –õ–ò–ù–ï–ô–ù–´–ï –ì–†–ê–§–ò–ö–ò ----------
axes[0].plot(epochs_range, smooth_train_acc,  label='–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å')
axes[0].plot(epochs_range, smooth_val_acc,    label='–í–∞–ª–∏–¥–∏—Ä—É—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å')
axes[0].set_xlabel('–≠–ø–æ—Ö–∞')
axes[0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
axes[0].set_title('–î–∏–Ω–∞–º–∏–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ (—Å–≥–ª–∞–∂–µ–Ω–Ω–∞—è)')
axes[0].grid(alpha=0.3)
axes[0].legend(loc='lower right')

# --------- –õ–û–ì–ê–†–ò–§–ú–ò–ß–ï–°–ö–ò–ï/¬´–ì–ò–ü–ï–†–ë–û–õ–ò–ß–ï–°–ö–ò–ï¬ª –ì–†–ê–§–ò–ö–ò –ü–û–¢–ï–†–¨ ----------
axes[1].plot(epochs_range, smooth_train_loss, label='–û–±—É—á–∞—é—â–∏–µ –ø–æ—Ç–µ—Ä–∏')
axes[1].plot(epochs_range, smooth_val_loss,   label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏')
axes[1].set_xlabel('–≠–ø–æ—Ö–∞')
axes[1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
axes[1].set_title('–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ—Ç–µ—Ä—å (log-scale)')
axes[1].set_yscale('log')   # –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞ ‚Äì –≥—Ä–∞—Ñ–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç—Å—è ¬´–≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π¬ª
axes[1].grid(alpha=0.3)
axes[1].legend(loc='upper right')

plt.tight_layout()
plt.show()

# ===============================================
# 10. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–†–ò–ú–ï–†–ê–• –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –û–¢–ó–´–í–û–í
# ===============================================
student_reviews = [
    "–õ–µ–∫—Ü–∏—è –±—ã–ª–∞ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π, –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –æ—Ç–ª–∏—á–Ω–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª!",
    "–°–ª–æ–∂–Ω–æ –ø–æ–Ω—è—Ç—å —Ç–µ–º—É, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –ø—Ä–∞–∫—Ç–∏–∫–∏",
    "–û–±—ã—á–Ω–∞—è –ª–µ–∫—Ü–∏—è, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ, —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞–Ω–∏—è",
    "–û—Ç–ª–∏—á–Ω—ã–π –∫—É—Ä—Å! –û—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞",
    "–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –∑–∞–ø–∞–∑–¥—ã–≤–∞–µ—Ç, –º–∞—Ç–µ—Ä–∏–∞–ª –ø–æ–¥–∞–µ—Ç—Å—è —Å–∫—É—á–Ω–æ"
]

print("\nüéì –ê–ù–ê–õ–ò–ó –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –û–¢–ó–´–í–û–í –û –õ–ï–ö–¶–ò–Ø–•:")
for i, review in enumerate(student_reviews, 1):
    seq = text_to_sequence(preprocess_text(review), vocab, MAX_LEN)
    seq = np.array([seq])
    pred = model.predict(seq, verbose=0)
    emotion = label_map[np.argmax(pred)]
    confidence = np.max(pred) * 100

    print(f"{i}. '{review}' ‚Üí {emotion.upper()} ({confidence:.1f}%)")

# ===============================================
# 11. HTML –û–¢–ß–ï–¢ (–°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í)
# ===============================================
html_report = f"""
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —ç—Å—Å–µ | TensorFlow –ù–°</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        .metric-card {{ transition: transform 0.3s; }}
        .metric-card:hover {{ transform: translateY(-5px); }}
    </style>
</head>
<body class="bg-gradient-to-br from-blue-50 to-indigo-100 min-h-screen p-8">
    <div class="max-w-6xl mx-auto">
        <header class="text-center mb-12">
            <h1 class="text-5xl font-bold bg-gradient-to-r from-purple-600 to-blue-600
                       bg-clip-text text-transparent mb-4">
                üéì –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏
            </h1>
            <h2 class="text-2xl text-gray-700 mb-2">
                –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å TensorFlow
            </h2>
            <p class="text-xl text-gray-600">–°—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–µ —ç—Å—Å–µ –∏ –æ—Ç–∑—ã–≤—ã –æ –ª–µ–∫—Ü–∏—è—Ö</p>
            <div class="flex justify-center gap-4 mt-6 text-sm text-gray-500">
                <span>–¢–æ—á–Ω–æ—Å—Ç—å: <strong>{test_accuracy:.3f}</strong></span>
                <span>–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df_sample):,}</span>
                <span>–°–ª–æ–≤–∞—Ä—å: {VOCAB_SIZE:,}</span>
            </div>
        </header>

        <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6 mb-12">
            <div class="metric-card bg-white p-8 rounded-2xl shadow-xl border border-gray-100">
                <div class="text-3xl mb-2">üéØ</div>
                <h3 class="text-xl font-semibold text-gray-800 mb-2">–¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å</h3>
                <div class="text-4xl font-bold text-green-600">{test_accuracy:.3f}</div>
            </div>
            <div class="metric-card bg-white p-8 rounded-2xl shadow-xl border border-gray-100">
                <div class="text-3xl mb-2">üìö</div>
                <h3 class="text-xl font-semibold text-gray-800 mb-2">–û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤</h3>
                <div class="text-4xl font-bold text-blue-600">{len(X_train):,}</div>
            </div>
            <div class="metric-card bg-white p-8 rounded-2xl shadow-xl border border-gray-100">
                <div class="text-3xl mb-2">üß†</div>
                <h3 class="text-xl font-semibold text-gray-800 mb-2">–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏</h3>
                <div class="text-4xl font-bold text-purple-600">{model.count_params():,}</div>
            </div>
        </div>

        <div class="grid grid-cols-1 lg:grid-cols-2 gap-8 mb-12">
            <div class="bg-white p-8 rounded-2xl shadow-xl">
                <h3 class="text-2xl font-bold mb-6">üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö –æ—Ç–∑—ã–≤–∞—Ö</h3>
"""

for i, review in enumerate(student_reviews, 1):
    seq = text_to_sequence(preprocess_text(review), vocab, MAX_LEN)
    seq = np.array([seq])
    pred = model.predict(seq, verbose=0)
    emotion = label_map[np.argmax(pred)]
    confidence = np.max(pred) * 100

    color = "bg-red-100 text-red-800" if emotion == "negative" else \
            "bg-yellow-100 text-yellow-800" if emotion == "neutral" else \
            "bg-green-100 text-green-800"

    html_report += f"""
                <div class="mb-6 p-4 border-l-4 {color} border border-gray-200 rounded-r-lg">
                    <div class="font-semibold text-lg mb-1">–û—Ç–∑—ã–≤ {i}</div>
                    <div class="text-gray-700 mb-3">"{review}"</div>
                    <div class="flex items-center gap-4">
                        <span class="px-4 py-1 rounded-full text-sm font-bold {color.split()[0]}">
                            {emotion.upper()}
                        </span>
                        <span class="text-sm text-gray-600">–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1f}%</span>
                    </div>
                </div>
    """

html_report += """
            </div>

            <div class="bg-white p-8 rounded-2xl shadow-xl">
                <h3 class="text-2xl font-bold mb-6">‚öôÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏</h3>
                <div class="space-y-3 text-sm">
"""

html_report += """
            <div class="bg-white p-8 rounded-2xl shadow-xl">
                <h3 class="text-2xl font-bold mb-6">‚öôÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏</h3>
                <div class="space-y-3 text-sm">
"""

for layer in model.layers:
    try:
        out_shape = tuple(layer.output.shape)
    except Exception:
        out_shape = "unknown"

    html_report += f"""
                    <div class="flex justify-between p-3 bg-gray-50 rounded-lg">
                        <span>{layer.name}</span>
                        <span class="font-mono bg-blue-100 px-2 py-1 rounded text-xs">{out_shape}</span>
                    </div>
    """

html_report += """
                </div>
            </div>
        </div>
"""


html_report += """
                </div>
            </div>
        </div>

        <div class="bg-white p-8 rounded-2xl shadow-xl">
            <h3 class="text-2xl font-bold mb-6">üìà –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç</h3>
            <div class="overflow-x-auto">
                <table class="w-full text-sm">
                    <thead class="bg-gray-50">
                        <tr>
                            <th class="p-3 text-left font-semibold">–ö–ª–∞—Å—Å</th>
                            <th class="p-3 text-left font-semibold">Precision</th>
                            <th class="p-3 text-left font-semibold">Recall</th>
                            <th class="p-3 text-left font-semibold">F1-score</th>
                        </tr>
                    </thead>
                    <tbody class="divide-y divide-gray-200">
"""

report = classification_report(y_true, y_pred,
                              target_names=['negative', 'neutral', 'positive'],
                              output_dict=True)

for label in ['negative', 'neutral', 'positive']:
    html_report += f"""
                        <tr>
                            <td class="p-3 font-medium">{label}</td>
                            <td class="p-3">{report[label]['precision']:.3f}</td>
                            <td class="p-3">{report[label]['recall']:.3f}</td>
                            <td class="p-3 font-semibold">{report[label]['f1-score']:.3f}</td>
                        </tr>
"""

html_report += """
                    </tbody>
                </table>
            </div>
        </div>

        <footer class="text-center mt-16 text-gray-500 text-sm">
            <p>–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —ç—Å—Å–µ –∏ –æ—Ç–∑—ã–≤–æ–≤ –æ –ª–µ–∫—Ü–∏—è—Ö</p>
            <p>TensorFlow Dense Neural Network | Google Colab | 2025</p>
        </footer>
    </div>
</body>
</html>
"""

# –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –æ—Ç—á–µ—Ç
with open('sentiment_analysis_report.html', 'w', encoding='utf-8') as f:
    f.write(html_report)

print("\n‚úÖ HTML –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: sentiment_analysis_report.html")
print("üìÅ –°–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞!")

# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
def predict_emotion(text):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    clean_text = preprocess_text(text)
    seq = text_to_sequence(clean_text, vocab, MAX_LEN)
    seq = np.array([seq])
    pred = model.predict(seq, verbose=0)
    emotion = label_map[np.argmax(pred)]
    confidence = np.max(pred) * 100
    return emotion, confidence

print("\nüéØ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï:")
test_text = input("–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ –æ –ª–µ–∫—Ü–∏–∏: ")
emotion, conf = predict_emotion(test_text)
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {emotion.upper()} ({conf:.1f}%)")

print("\nüéâ –ü–†–û–ï–ö–¢ –í–´–ü–û–õ–ù–ï–ù –£–°–ü–ï–®–ù–û!")
print("‚úì –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –ù–° TensorFlow")
print("‚úì –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
print("‚úì –ü–æ–ª–Ω–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã")
print("‚úì –ê–Ω–∞–ª–∏–∑ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
print("‚úì HTML –æ—Ç—á–µ—Ç —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π")
print("‚úì –ì–æ—Ç–æ–≤–æ –¥–ª—è Google Colab!")
